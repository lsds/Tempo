import textwrap
from collections.abc import Callable, Iterable, Sequence
from typing import TypeVar

from tempo.core.dataflow_graph_interface import DataflowGraphI
from tempo.core.datatypes import BackendTensorT, OpId, OpInId, OpOutId
from tempo.core.dependence_graph import PDG, OpWithIO
from tempo.core.dtype import DataType
from tempo.core.shape import Shape
from tempo.core.tensor_op import TensorOp
from tempo.core.thunk import ThunkExecutionCtx
from tempo.utils import logger

DataT = TypeVar("DataT")

log = logger.get_logger(__name__)


class DataflowGraph(DataflowGraphI[BackendTensorT]):
    def __init__(
        self,
        subgraph: PDG,
        # irouter[0] = ((1, 0), (2, 1)) means that
        # input 0 is used by op 1 at its input[0] and op 2 at its input[1]
        irouter: tuple[tuple[tuple[OpId, OpInId], ...], ...],
        # example: orouter[0] = (5, 1) means that output tensor 0 is generated by op 5
        # at its output[1] (op 5 may have multiple outputs)
        orouter: tuple[tuple[OpId, OpOutId], ...],
    ) -> None:
        super().__init__(irouter, orouter)
        self.subgraph = subgraph

        self.orouter_idx_map: dict[OpId, dict[OpOutId, int]] = {}
        for idx, (o_id, o_out) in enumerate(self.orouter):
            inner_map = self.orouter_idx_map.setdefault(o_id, {})
            inner_map[o_out] = idx

        self.orouter_op_ids = {o_id for o_id, _ in self.orouter}

    # NOTE: Lazily initialized properties because dataflows are used in grouping.
    @property
    def topologically_sorted_nodes(self) -> list[TensorOp]:
        """Lazily initialized list of topologically sorted nodes."""
        if not hasattr(self, "_topologically_sorted_nodes"):
            self._topologically_sorted_nodes = list(self.subgraph.nodes_topologically_sorted)
        return self._topologically_sorted_nodes

    @property
    def op_id_to_op_with_io(self) -> dict[OpId, OpWithIO]:
        """Lazily initialized mapping from op ID to op with I/O information."""
        if not hasattr(self, "_op_id_to_op_with_io"):
            self._op_id_to_op_with_io = {
                op_io.op.op_id: op_io for op_io in self.subgraph.nodes_with_io
            }
        return self._op_id_to_op_with_io

    def __str__(self) -> str:
        return f"DataflowGraph({str(self.subgraph)})"

    def _build_route_map(self) -> dict[OpId, dict[OpInId, str]]:
        # Setup a map to track SSA IDs for each op's inputs, similar to top_with_input_values
        # This map will be used during code generation to manage SSA variable names
        route_map: dict[OpId, dict[OpInId, str]] = {
            op.op_id: {} for op in self.topologically_sorted_nodes
        }

        # Add input routing - desugar into SSA variables
        for idx in range(self.num_inputs):
            new_var_name = f"inputs_{idx}"
            ops_dependent_on_input = self.irouter[idx]
            for op_id, input_idx in ops_dependent_on_input:
                # Store the mapping in our SSA ID map
                route_map[op_id][input_idx] = new_var_name

        # Execute operations in topological order
        for op in self.topologically_sorted_nodes:
            op_outputs = [f"op_{op.op_id}_{out_idx}" for out_idx in range(op.num_outputs)]
            io = self.op_id_to_op_with_io[op.op_id]
            # Route outputs to dependent operations using our SSA ID map
            for out_op, dep_data in io.outputs:
                local_out = op_outputs[dep_data.src_out_idx]
                route_map[out_op.op_id][dep_data.sink_in_idx] = local_out

        return route_map

    def _format_tuple(self, elements: list[str]) -> str:
        """Format a list of elements as a Python tuple string.

        Args:
            elements: List of element strings to format

        Returns:
            Properly formatted tuple string:
            - Empty: "()"
            - Single element: "(element,)"
            - Multiple elements: "(element1, element2)"
        """
        if len(elements) == 0:
            return "()"
        elif len(elements) == 1:
            return f"({elements[0]},)"
        else:
            return f"({', '.join(elements)})"

    def _generate_executor_function(self, fn_name: str) -> str:
        """Generate a Python function string that executes the computation
         without interpretation overhead.

        This function will have the thunk_map as a local variable and
        execute operations in topological order,
        with early deletion of intermediate values that are no longer needed.

        Args:
            fn_name: The name for the generated function

        Returns:
            A string containing the generated Python function
        """

        results: list[str] = [None] * len(self.orouter)  # type: ignore

        lines = []

        route_map = self._build_route_map()
        # Track which values are still needed after each operation
        last_users = self._compute_last_users_of_each_tensor(route_map)

        # Add input routing - desugar into SSA variables
        ins_desugared = [f"inputs_{idx}" for idx in range(self.num_inputs)]
        df_input_tuple = self._format_tuple(ins_desugared)
        lines.append(f"{df_input_tuple} = inputs")
        lines.append("del inputs")

        # Execute operations in topological order
        for op in self.topologically_sorted_nodes:
            # Collect inputs for this operation using SSA variables from our map
            op_inputs = [route_map[op.op_id][OpInId(i)] for i in range(op.num_inputs)]
            input_tuple = self._format_tuple(op_inputs)

            op_outputs = [f"op_{op.op_id}_{out_idx}" for out_idx in range(op.num_outputs)]
            output_tuple = self._format_tuple(op_outputs)

            comment = f"{str(op)}"
            lines.append(f"{output_tuple}  = thunk_{op.op_id}({input_tuple}, exec_ctx) # {comment}")

            if op.op_id in self.orouter_op_ids:
                for o_idx, idx in self.orouter_idx_map[op.op_id].items():
                    results[idx] = op_outputs[o_idx]

            # Early deletion of values no longer needed
            if op.op_id in last_users:
                for var_name in last_users[op.op_id]:
                    lines.append(f"del {var_name}")

        lines.append(f"return {self._format_tuple(results)}")

        final_lines = []
        final_lines.append(f"def {fn_name}(inputs, exec_ctx):")
        for line in lines:
            final_lines.append("    " + line)

        return textwrap.dedent("\n".join(final_lines))

    def _compute_last_users_of_each_tensor(
        self, op_input_ids: dict[OpId, dict[OpInId, str]]
    ) -> dict[OpId, set[str]]:
        """Compute which intermediate values are no longer needed after each operation."""
        last_users = {}

        # Start from outputs and work backwards
        always_needed = set()
        for o_id, o_out in self.orouter:
            # Use the new SSA naming scheme
            always_needed.add(f"op_{o_id}_{o_out}")

        live_tensors = set()
        # Add the input tensors
        for idx in range(self.num_inputs):
            live_tensors.add(f"inputs_{idx}")

        for op in self.topologically_sorted_nodes:
            for out_idx in range(op.num_outputs):
                live_tensors.add(f"op_{op.op_id}_{out_idx}")

        live_tensors.difference_update(always_needed)

        # Work backwards through the graph
        for op in reversed(self.topologically_sorted_nodes):
            tensors_used = set(op_input_ids[op.op_id].values())

            live_tensors_eliminated = live_tensors.intersection(tensors_used)
            live_tensors.difference_update(live_tensors_eliminated)

            last_users[op.op_id] = live_tensors_eliminated

        if live_tensors:
            # NOTE: If there are any live tensors, these are produced but not used by any op.
            # Delete them right after being produced.
            for remaining_live_tensor in live_tensors:
                produced_id = remaining_live_tensor.split("_")[1]
                last_user_set: set[str] = last_users[OpId(int(produced_id))]
                last_user_set.add(remaining_live_tensor)

        return last_users

    def get_compiled_executor(
        self,
        op_id: OpId,
        thunk_map: dict[
            OpId,
            Callable[[tuple[BackendTensorT, ...], ThunkExecutionCtx], tuple[BackendTensorT, ...]],
        ],
    ) -> Callable[[tuple[BackendTensorT, ...], ThunkExecutionCtx], tuple[BackendTensorT, ...]]:
        """Compile the generated executor function and return a callable.

        Args:
            thunk_map: Dictionary mapping OpId to thunk functions

        Returns:
            A compiled function that can execute the computation
        """
        fn_name = f"dataflow_{op_id}"
        func_str = self._generate_executor_function(fn_name)

        log.debug("Generated executor function for dataflow: \n %s", func_str)

        # print("-" * 100)
        # print("Tempo compiled the following function:")
        # print(func_str)
        # print("-" * 100)

        try:
            # Execute the code to define the function
            mod = compile(func_str, "<tempo_codegen_dataflow>", "exec", optimize=2)

            namespace = {f"thunk_{op_id}": thunk_map[op_id] for op_id in thunk_map}
            exec(mod, namespace)
            fn = namespace[fn_name]

            return fn  # type: ignore
        except Exception as e:
            log.error("Failed to compile generated executor: %s", e)
            log.error("Generated code:\n%s", func_str)
            raise

    def _simulate_graph_execution(  # noqa: C901
        self,
        input_values: tuple[DataT, ...],
        user_func: Callable[[TensorOp, tuple[DataT, ...]], tuple[DataT, ...]],
    ) -> tuple[DataT, ...]:
        """This is essentially an abstract dataflow graph executor. It assumes basis dependencies
        between ops, and executes them in topological order.

        Args:
            input_values (Tuple[DataT, ...]): The starting values for graph interpretation
            infer_func (Callable[ [TensorOp, Tuple[DataT, ...]], Tuple[DataT, ...] ]):
              The function to call on each op during graph interpretation

        Returns:
            Tuple[DataT, ...]: The results of the abstract graph interpretation

        """
        # TODO improve the performance/cleanliness of this method?

        # We will use this map to store the inputs to each op
        top_with_input_values: dict[OpId, dict[OpInId, DataT]] = {
            o.op_id: {} for o in self.topologically_sorted_nodes
        }
        # Route the inputs to the dataflow using the irouter to the internal ops
        for idx in range(self.num_inputs):
            ops_dependent_on_input = self.irouter[idx]
            for op_id, input_idx in ops_dependent_on_input:
                top_with_input_values[op_id][input_idx] = input_values[idx]

        result: list[DataT] = [None] * len(self.orouter)  # type: ignore
        # Start traversing the graph in topological order
        for op in self.topologically_sorted_nodes:
            # Collect this op's inputs
            num_ins = len(top_with_input_values[op.op_id])
            input_values_ = tuple(
                top_with_input_values[op.op_id][OpInId(i)] for i in range(num_ins)
            )
            # invoke the user function
            try:
                inferred_output_values = user_func(op, input_values_)
            except Exception as e:
                raise RuntimeError(
                    f"Error while executing op {op.op_id} in dataflow with inputs {input_values}"
                ) from e
            io = self.op_id_to_op_with_io[op.op_id]
            # Route the outputs of this op to the inputs of any dependent ops
            for out_op, dep_data in io.outputs:
                local_out = inferred_output_values[dep_data.src_out_idx]
                top_with_input_values[out_op.op_id][dep_data.sink_in_idx] = local_out

            if op.op_id in self.orouter_op_ids:
                for o_idx, idx in self.orouter_idx_map[op.op_id].items():
                    result[idx] = inferred_output_values[o_idx]

        return tuple(result)

    def execute(
        self,
        inputs: tuple[BackendTensorT, ...],
        thunk_map: Callable[[TensorOp, tuple[BackendTensorT, ...]], tuple[BackendTensorT, ...]],
    ) -> tuple[BackendTensorT, ...]:
        return self._simulate_graph_execution(inputs, thunk_map)

    def infer_output_shapes(self, input_shapes: Sequence[Shape]) -> Sequence[Shape]:
        return self._simulate_graph_execution(
            tuple(input_shapes), lambda op, shapes: tuple(op.infer_output_shapes(shapes))
        )

    def infer_output_dtypes(self, input_dtypes: Sequence[DataType]) -> Sequence[DataType]:
        return self._simulate_graph_execution(
            tuple(input_dtypes), lambda op, dtypes: tuple(op.infer_output_dtypes(dtypes))
        )

    @property
    def nodes(self) -> Iterable[TensorOp]:
        return self.subgraph.nodes  # type: ignore
