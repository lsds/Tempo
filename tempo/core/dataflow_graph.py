from typing import Callable, Dict, Iterable, List, Sequence, Tuple, TypeVar

from tempo.core.dataflow_graph_interface import DataflowGraphI
from tempo.core.datatypes import BackendTensorT, OpId, OpInId, OpOutId
from tempo.core.dependence_graph import PDG
from tempo.core.dtype import DataType
from tempo.core.shape import Shape
from tempo.core.tensor_op import TensorOp
from tempo.utils import logger

DataT = TypeVar("DataT")

log = logger.get_logger(__name__)


class DataflowGraph(DataflowGraphI[BackendTensorT]):
    def __init__(
        self,
        subgraph: PDG,
        irouter: Tuple[Tuple[Tuple[OpId, OpInId], ...], ...],
        # example: orouter[0] = (5, 1) means that output tensor 0 is generated by op 5
        # at its output[1] (op 5 may have multiple outputs)
        orouter: Tuple[Tuple[OpId, OpOutId], ...],
    ) -> None:
        super().__init__(irouter, orouter)
        self.subgraph = subgraph
        try:
            self.topologically_sorted_nodes = list(self.subgraph.nodes_topologically_sorted)
        except Exception as e:
            raise RuntimeError(
                f"Error while topologically sorting nodes in dataflow {self.subgraph}"
            ) from e
        self.op_id_to_op_with_io = {op_io.op.op_id: op_io for op_io in self.subgraph.nodes_with_io}

        self.orouter_idx_map: Dict[OpId, Dict[OpOutId, int]] = {}
        for idx, (o_id, o_out) in enumerate(self.orouter):
            inner_map = self.orouter_idx_map.setdefault(o_id, {})
            inner_map[o_out] = idx

        self.orouter_op_ids = {o_id for o_id, _ in self.orouter}

    def __str__(self) -> str:
        return f"DataflowGraph({str(self.subgraph)})"

    def _simulate_graph_execution(  # noqa: C901
        self,
        input_values: Tuple[DataT, ...],
        user_func: Callable[[TensorOp, Tuple[DataT, ...]], Tuple[DataT, ...]],
    ) -> Tuple[DataT, ...]:
        """This is essentially an abstract dataflow graph executor. It assumes basis dependencies
        between ops, and executes them in topological order.

        Args:
            input_values (Tuple[DataT, ...]): The starting values for graph interpretation
            infer_func (Callable[ [TensorOp, Tuple[DataT, ...]], Tuple[DataT, ...] ]):
              The function to call on each op during graph interpretation

        Returns:
            Tuple[DataT, ...]: The results of the abstract graph interpretation

        """
        # TODO improve the performance/cleanliness of this method?

        # We will use this map to store the inputs to each op
        top_with_input_values: Dict[OpId, Dict[OpInId, DataT]] = {
            o.op_id: {} for o in self.topologically_sorted_nodes
        }
        # Route the inputs to the dataflow using the irouter to the internal ops
        for idx in range(self.num_inputs):
            ops_dependent_on_input = self.irouter[idx]
            for op_id, input_idx in ops_dependent_on_input:
                top_with_input_values[op_id][input_idx] = input_values[idx]

        result: List[DataT] = [None] * len(self.orouter)  # type: ignore
        # Start traversing the graph in topological order
        for op in self.topologically_sorted_nodes:
            # Collect this op's inputs
            num_ins = len(top_with_input_values[op.op_id])
            input_values_ = tuple(
                top_with_input_values[op.op_id][OpInId(i)] for i in range(num_ins)
            )
            # invoke the user function
            try:
                inferred_output_values = user_func(op, input_values_)
            except Exception as e:
                raise RuntimeError(
                    f"Error while executing op {op.op_id} in dataflow with inputs {input_values}"
                ) from e
            io = self.op_id_to_op_with_io[op.op_id]
            # Route the outputs of this op to the inputs of any dependent ops
            for out_op, dep_data in io.outputs:
                local_out = inferred_output_values[dep_data.src_out_idx]
                top_with_input_values[out_op.op_id][dep_data.sink_in_idx] = local_out

            if op.op_id in self.orouter_op_ids:
                for o_idx, idx in self.orouter_idx_map[op.op_id].items():
                    result[idx] = inferred_output_values[o_idx]

        return tuple(result)

    def execute(
        self,
        inputs: Tuple[BackendTensorT, ...],
        thunk_map: Callable[[TensorOp, Tuple[BackendTensorT, ...]], Tuple[BackendTensorT, ...]],
    ) -> Tuple[BackendTensorT, ...]:
        return self._simulate_graph_execution(inputs, thunk_map)

    def infer_output_shapes(self, input_shapes: Sequence[Shape]) -> Sequence[Shape]:
        return self._simulate_graph_execution(
            tuple(input_shapes), lambda op, shapes: tuple(op.infer_output_shapes(shapes))
        )

    def infer_output_dtypes(self, input_dtypes: Sequence[DataType]) -> Sequence[DataType]:
        return self._simulate_graph_execution(
            tuple(input_dtypes), lambda op, dtypes: tuple(op.infer_output_dtypes(dtypes))
        )

    @property
    def nodes(self) -> Iterable[TensorOp]:
        return self.subgraph.nodes  # type: ignore
